[
  {
    "objectID": "lab06.html",
    "href": "lab06.html",
    "title": "ess-330-lab06",
    "section": "",
    "text": "options(repos = c(CRAN = \"https://cloud.r-project.org/\"))\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\ninstall.packages(\"powerjoin\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jn/hk4fnzlx679cs3m0_lnzh7hh0000gn/T//RtmprarCNN/downloaded_packages\n\nlibrary(powerjoin)\ninstall.packages(\"glue\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jn/hk4fnzlx679cs3m0_lnzh7hh0000gn/T//RtmprarCNN/downloaded_packages\n\nlibrary(glue)\ninstall.packages(\"vip\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jn/hk4fnzlx679cs3m0_lnzh7hh0000gn/T//RtmprarCNN/downloaded_packages\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\ninstall.packages(\"baguette\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jn/hk4fnzlx679cs3m0_lnzh7hh0000gn/T//RtmprarCNN/downloaded_packages\n\nlibrary(baguette)\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')"
  },
  {
    "objectID": "lab06.html#my-turn",
    "href": "lab06.html#my-turn",
    "title": "ess-330-lab06",
    "section": "My Turn!",
    "text": "My Turn!\n\ninstall.packages(\"xgboost\")\n\n\nThe downloaded binary packages are in\n    /var/folders/jn/hk4fnzlx679cs3m0_lnzh7hh0000gn/T//RtmprarCNN/downloaded_packages\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nxg_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\") \n\nxg_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(xg_model) %&gt;%\n  fit(data = camels_train)\n\nnn_model &lt;- mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\") \n\nnn_wf &lt;- workflow() %&gt;%\n  add_recipe(rec) %&gt;%\n  add_model(nn_model) %&gt;%\n  fit(data = camels_train)\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model, xg_model, nn_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_mlp        Prepro… rmse    0.565  0.0312    10 recipe       mlp       1\n2 recipe_mlp        Prepro… rsq     0.762  0.0282    10 recipe       mlp       1\n3 recipe_linear_reg Prepro… rmse    0.601  0.0350    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.737  0.0302    10 recipe       line…     2\n5 recipe_rand_fore… Prepro… rmse    0.601  0.0320    10 recipe       rand…     3\n6 recipe_rand_fore… Prepro… rsq     0.737  0.0302    10 recipe       rand…     3\n7 recipe_boost_tree Prepro… rmse    0.634  0.0294    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.715  0.0275    10 recipe       boos…     4\n\n\nAfter evaluating the models and comparing them, I would choose to move forward with the neural network model. It’s rsq was the highest of the four and it had the lowest rmse, indicating that it is the most effective model."
  },
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "ESS330 Lab_08",
    "section": "",
    "text": "Data Import/Tidy/Transform\n\noptions(repos = c(CRAN = \"https://cloud.r-project.org/\"))\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(skimr)\nlibrary(visdat)\nlibrary(ggpubr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ranger)\nlibrary(dials)\nlibrary(patchwork)\n\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\nglimpse(camels)\n\nRows: 671\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"djf\", \"mam\", \"mam\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4489279, 0.308…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.17972945, 0.16461821, 0.28701001, 0.44386282, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.052140094, 0.02625797…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0747, 0.0522, 0.0711, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -14.8410, -14.4819,…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 45.17501, 44.86920,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -69.31470, -69.9…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 247.80, 310.38, 615.70, 47…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 29.56035, 49.92122,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 769.05, 909.10, 383.82,…\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 766.53, 904.94, 396.10,…\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9548, 0.9906, 1.0000, 1…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 4.903259, 5.086811,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 3.990843, 4.300978,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.8706685, 0.891…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.3986194, 0.445…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 1.0000000, 0.850…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2500000, 0.2410270, 0.225615…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.400000, 2.340180, 2.237435, 2…\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n\nvis_miss(camels)\n\n\n\n\n\n\n\ncamels_clean &lt;- camels %&gt;% \n  distinct() %&gt;%\n  drop_na()\n\nskim(camels_clean)\n\n\nData summary\n\n\nName\ncamels_clean\n\n\nNumber of rows\n507\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1\n8\n8\n0\n507\n0\n\n\nhigh_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n0\n1\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1\n3.15\n1.49\n0.64\n2.20\n3.05\n3.70\n8.94\n▅▇▂▁▁\n\n\npet_mean\n0\n1\n2.80\n0.56\n1.94\n2.37\n2.70\n3.16\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1\n-0.06\n0.56\n-1.44\n-0.41\n0.08\n0.27\n0.92\n▂▃▃▇▃\n\n\nfrac_snow\n0\n1\n0.19\n0.21\n0.00\n0.04\n0.11\n0.26\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1\n1.13\n0.67\n0.22\n0.73\n0.89\n1.45\n5.21\n▇▃▁▁▁\n\n\nhigh_prec_freq\n0\n1\n21.00\n4.80\n7.90\n18.45\n22.20\n24.55\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1\n1.37\n0.19\n1.10\n1.22\n1.32\n1.47\n2.07\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1\n256.92\n36.76\n169.90\n233.57\n259.15\n286.75\n348.70\n▂▅▇▆▁\n\n\nlow_prec_dur\n0\n1\n6.29\n3.46\n2.79\n4.42\n5.14\n7.21\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1\n0.73\n0.19\n0.30\n0.58\n0.76\n0.90\n1.00\n▂▅▅▆▇\n\n\nglim_2nd_class_frac\n0\n1\n0.19\n0.13\n0.00\n0.08\n0.19\n0.30\n0.49\n▇▆▆▅▂\n\n\ncarbonate_rocks_frac\n0\n1\n0.13\n0.26\n0.00\n0.00\n0.00\n0.09\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n0\n1\n0.12\n0.06\n0.01\n0.07\n0.12\n0.17\n0.28\n▆▆▇▆▁\n\n\ngeol_permeability\n0\n1\n-13.86\n1.13\n-16.50\n-14.66\n-13.90\n-13.03\n-10.97\n▂▃▇▅▁\n\n\nsoil_depth_pelletier\n0\n1\n9.45\n15.25\n0.27\n1.00\n1.20\n6.99\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1\n1.28\n0.27\n0.40\n1.08\n1.42\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.59\n▁▇▂▁▁\n\n\nsoil_conductivity\n0\n1\n1.67\n1.39\n0.45\n0.89\n1.34\n1.89\n10.91\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1\n0.52\n0.15\n0.09\n0.41\n0.53\n0.64\n0.88\n▁▃▆▇▁\n\n\nsand_frac\n0\n1\n35.69\n15.47\n8.18\n24.62\n34.60\n43.77\n91.16\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1\n33.30\n12.82\n4.13\n23.53\n33.65\n42.76\n67.77\n▂▇▇▅▁\n\n\nclay_frac\n0\n1\n20.35\n9.81\n2.08\n13.82\n18.88\n26.58\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1\n0.02\n0.15\n0.00\n0.00\n0.00\n0.00\n1.71\n▇▁▁▁▁\n\n\norganic_frac\n0\n1\n0.45\n2.97\n0.00\n0.00\n0.00\n0.00\n39.37\n▇▁▁▁▁\n\n\nother_frac\n0\n1\n10.88\n16.94\n0.00\n0.00\n2.25\n15.48\n89.87\n▇▂▁▁▁\n\n\ngauge_lat\n0\n1\n39.55\n5.21\n27.05\n36.23\n39.35\n43.96\n48.66\n▂▃▇▅▆\n\n\ngauge_lon\n0\n1\n-98.08\n16.26\n-124.39\n-112.03\n-97.04\n-83.40\n-67.94\n▇▆▇▇▅\n\n\nelev_mean\n0\n1\n842.40\n824.66\n21.75\n278.50\n492.06\n1055.30\n3457.46\n▇▂▁▁▁\n\n\nslope_mean\n0\n1\n50.12\n48.29\n0.82\n7.97\n36.17\n77.62\n255.69\n▇▃▂▁▁\n\n\narea_gages2\n0\n1\n854.89\n1829.26\n4.03\n151.12\n383.82\n855.14\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1\n870.19\n1837.52\n4.10\n164.23\n397.25\n861.21\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1\n0.61\n0.38\n0.00\n0.20\n0.75\n0.97\n1.00\n▅▁▂▂▇\n\n\nlai_max\n0\n1\n3.00\n1.52\n0.37\n1.63\n2.75\n4.60\n5.58\n▅▇▃▅▇\n\n\nlai_diff\n0\n1\n2.27\n1.32\n0.15\n1.07\n2.06\n3.49\n4.82\n▇▇▆▃▆\n\n\ngvf_max\n0\n1\n0.70\n0.17\n0.18\n0.58\n0.75\n0.86\n0.92\n▁▂▃▃▇\n\n\ngvf_diff\n0\n1\n0.31\n0.15\n0.03\n0.18\n0.29\n0.45\n0.65\n▅▇▅▇▂\n\n\ndom_land_cover_frac\n0\n1\n0.80\n0.19\n0.31\n0.64\n0.84\n0.99\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n0\n1\n0.18\n0.03\n0.12\n0.16\n0.18\n0.19\n0.25\n▃▅▇▂▂\n\n\nroot_depth_99\n0\n1\n1.81\n0.30\n1.50\n1.51\n1.79\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n0\n1\n1.46\n1.61\n0.00\n0.49\n1.00\n1.72\n9.50\n▇▁▁▁▁\n\n\nrunoff_ratio\n0\n1\n0.38\n0.24\n0.00\n0.22\n0.34\n0.51\n1.36\n▇▇▃▁▁\n\n\nslope_fdc\n0\n1\n1.19\n0.53\n0.00\n0.81\n1.24\n1.56\n2.50\n▃▆▇▆▁\n\n\nbaseflow_index\n0\n1\n0.49\n0.17\n0.01\n0.39\n0.51\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n0\n1\n1.86\n0.80\n-0.64\n1.33\n1.69\n2.25\n6.24\n▁▇▃▁▁\n\n\nq5\n0\n1\n0.17\n0.25\n0.00\n0.01\n0.08\n0.22\n1.77\n▇▁▁▁▁\n\n\nq95\n0\n1\n4.98\n5.23\n0.00\n1.67\n3.46\n6.29\n31.23\n▇▂▁▁▁\n\n\nhigh_q_freq\n0\n1\n27.15\n30.51\n0.00\n6.92\n16.15\n38.60\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n0\n1\n7.62\n10.98\n0.00\n1.87\n3.24\n8.82\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n0\n1\n111.13\n86.38\n0.00\n35.35\n101.35\n173.10\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n0\n1\n23.38\n23.12\n0.00\n10.31\n16.92\n28.20\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n0\n1\n0.04\n0.13\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n0\n1\n185.49\n34.66\n112.25\n162.07\n177.80\n212.10\n287.75\n▂▇▅▃▁\n\n\n\n\nmap_qmean &lt;- ggplot(data = camels_clean, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"tomato\", high = \"lightgreen\") +\n  ggthemes::theme_map() +\n  ggtitle(\"Map of Site Q-Mean\") +\n  labs(color = \"q_mean\")\n\nprint(map_qmean)\n\n\n\n\n\n\n\n\n\n\nData Splitting\n\nset.seed(64)\n\ncamels_clean &lt;- camels_clean |&gt; \n  mutate(logQmean = log(q_mean))\n\ncc_split &lt;- initial_split(camels_clean, prop = 0.8)\ncc_train &lt;- training(cc_split)\ncc_test  &lt;- testing(cc_split)\n\n\n\nFeature Engineering\n\nrecipe_cc &lt;-  recipe(logQmean ~ aridity + p_mean, data = cc_train) %&gt;%\n  step_log(all_predictors()) %&gt;%\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  step_naomit(all_predictors(), all_outcomes())\n\ncc_baked_data &lt;- prep(recipe_cc, cc_train) |&gt; \n  bake(new_data = NULL)\n\n\n\nResampling and Model Testing\n\ncamels_cc_cv &lt;- vfold_cv(cc_train, v = 10)\n\nlm_model_cc &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\") \n\nlm_wf_cc &lt;- workflow() %&gt;%\n  add_recipe(recipe_cc) %&gt;%\n  add_model(lm_model_cc) %&gt;%\n  fit(data = cc_train) \n\nrf_model_cc &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf_cc &lt;- workflow() %&gt;%\n  add_recipe(recipe_cc) %&gt;%\n  add_model(rf_model_cc) %&gt;%\n  fit(data = cc_train)\n\nxg_model_cc &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\") \n\nxg_wf_cc &lt;- workflow() %&gt;%\n  add_recipe(recipe_cc) %&gt;%\n  add_model(xg_model_cc) %&gt;%\n  fit(data = cc_train)\n\nwf_cc &lt;- workflow_set(list(recipe_cc), list(lm_model_cc, rf_model_cc, xg_model_cc)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cc_cv) \n\nautoplot(wf_cc)\n\n\n\n\n\n\n\n\nAfter visualizing the model metrics, I selected the random forest model for my model tuning. The random forest model has the highest rsq value out of the three models, as well as having the lowest mean standard error. Random forest is a regression model, which I have chosen to run with the “ranger” engine. I think it will work well for this problems because it handles nonlinear relationships efficiently and reduces overfitting.\n\n\nModel Tuning\n\nrf_tuned_cc &lt;- rand_forest(\n  mtry = tune(),\n  trees = tune()\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf_cc &lt;- workflow() %&gt;%\n  add_recipe(recipe_cc) %&gt;%\n  add_model(rf_tuned_cc)\n\ndials &lt;- extract_parameter_set_dials(rf_tuned_cc) %&gt;%\n  finalize(cc_train)\n\ndials$object\n\n[[1]]\n# Randomly Selected Predictors (quantitative)\nRange: [1, 59]\n\n[[2]]\n# Trees (quantitative)\nRange: [1, 2000]\n\nset.seed(64)\nmy.grid &lt;- grid_space_filling(dials, size = 25)\n\n\nmodel_params &lt;-  tune_grid(\n    rf_wf_cc,\n    resamples = camels_cc_cv,\n    grid = my.grid,\n    metrics = metric_set(rmse, rsq, mae),\n    control = control_grid(save_pred = TRUE)\n  )\n\n→ A | warning: ! 5 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: ! 8 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\n\n\nThere were issues with some computations   A: x1\n→ C | warning: ! 10 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ D | warning: ! 13 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ E | warning: ! 15 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ F | warning: ! 17 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ G | warning: ! 20 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ H | warning: ! 22 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ I | warning: ! 25 columns were requested but there were 3 predictors in the data.\n               ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\n→ J | warning: ! 27 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ K | warning: ! 30 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ L | warning: ! 32 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ M | warning: ! 34 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ N | warning: ! 37 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ O | warning: ! 39 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ P | warning: ! 42 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ Q | warning: ! 44 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ R | warning: ! 46 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ S | warning: ! 49 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ T | warning: ! 51 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ U | warning: ! 54 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ V | warning: ! 56 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\n→ W | warning: ! 59 columns were requested but there were 3 predictors in the data.\n                ℹ 3 predictors will be used.\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\nThere were issues with some computations   A: x1   B: x1   C: x1   D: x1   E: x…\nThere were issues with some computations   A: x2   B: x2   C: x2   D: x2   E: x…\nThere were issues with some computations   A: x2   B: x2   C: x2   D: x2   E: x…\nThere were issues with some computations   A: x2   B: x2   C: x2   D: x2   E: x…\nThere were issues with some computations   A: x3   B: x3   C: x2   D: x2   E: x…\nThere were issues with some computations   A: x3   B: x3   C: x3   D: x3   E: x…\nThere were issues with some computations   A: x3   B: x3   C: x3   D: x3   E: x…\nThere were issues with some computations   A: x3   B: x3   C: x3   D: x3   E: x…\nThere were issues with some computations   A: x4   B: x4   C: x4   D: x4   E: x…\nThere were issues with some computations   A: x4   B: x4   C: x4   D: x4   E: x…\nThere were issues with some computations   A: x4   B: x4   C: x4   D: x4   E: x…\nThere were issues with some computations   A: x5   B: x4   C: x4   D: x4   E: x…\nThere were issues with some computations   A: x5   B: x5   C: x5   D: x5   E: x…\nThere were issues with some computations   A: x5   B: x5   C: x5   D: x5   E: x…\nThere were issues with some computations   A: x5   B: x5   C: x5   D: x5   E: x…\nThere were issues with some computations   A: x6   B: x6   C: x6   D: x5   E: x…\nThere were issues with some computations   A: x6   B: x6   C: x6   D: x6   E: x…\nThere were issues with some computations   A: x6   B: x6   C: x6   D: x6   E: x…\nThere were issues with some computations   A: x6   B: x6   C: x6   D: x6   E: x…\nThere were issues with some computations   A: x7   B: x7   C: x7   D: x7   E: x…\nThere were issues with some computations   A: x7   B: x7   C: x7   D: x7   E: x…\nThere were issues with some computations   A: x7   B: x7   C: x7   D: x7   E: x…\nThere were issues with some computations   A: x8   B: x7   C: x7   D: x7   E: x…\nThere were issues with some computations   A: x8   B: x8   C: x8   D: x8   E: x…\nThere were issues with some computations   A: x8   B: x8   C: x8   D: x8   E: x…\nThere were issues with some computations   A: x8   B: x8   C: x8   D: x8   E: x…\nThere were issues with some computations   A: x9   B: x9   C: x9   D: x9   E: x…\nThere were issues with some computations   A: x9   B: x9   C: x9   D: x9   E: x…\nThere were issues with some computations   A: x9   B: x9   C: x9   D: x9   E: x…\nThere were issues with some computations   A: x10   B: x9   C: x9   D: x9   E: …\nThere were issues with some computations   A: x10   B: x10   C: x10   D: x10   …\nThere were issues with some computations   A: x10   B: x10   C: x10   D: x10   …\nThere were issues with some computations   A: x10   B: x10   C: x10   D: x10   …\nThere were issues with some computations   A: x10   B: x10   C: x10   D: x10   …\n\nautoplot(model_params)\n\n\n\n\n\n\n\n\nAfter visualizing the results of the tuning process, I found that the mae, rmse, and rsq were pretty consistent for both the randomly selected predictor and the # of trees. This indicates that I can select a lower trees value and still get the same results as a higher value.\n\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n    mtry trees .metric .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1  1167 mae     standard   0.370    10  0.0225 Preprocessor1_Model01\n 2     1  1167 rmse    standard   0.582    10  0.0430 Preprocessor1_Model01\n 3     1  1167 rsq     standard   0.768    10  0.0283 Preprocessor1_Model01\n 4     3   584 mae     standard   0.374    10  0.0235 Preprocessor1_Model02\n 5     3   584 rmse    standard   0.587    10  0.0442 Preprocessor1_Model02\n 6     3   584 rsq     standard   0.767    10  0.0287 Preprocessor1_Model02\n 7     5  1666 mae     standard   0.373    10  0.0238 Preprocessor1_Model03\n 8     5  1666 rmse    standard   0.586    10  0.0446 Preprocessor1_Model03\n 9     5  1666 rsq     standard   0.768    10  0.0289 Preprocessor1_Model03\n10     8   167 mae     standard   0.374    10  0.0228 Preprocessor1_Model04\n# ℹ 65 more rows\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n   mtry trees .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     1  1167 mae     standard   0.370    10  0.0225 Preprocessor1_Model01\n2    30   334 mae     standard   0.373    10  0.0232 Preprocessor1_Model13\n3     5  1666 mae     standard   0.373    10  0.0238 Preprocessor1_Model03\n4    27  2000 mae     standard   0.373    10  0.0234 Preprocessor1_Model12\n5    13  1333 mae     standard   0.374    10  0.0235 Preprocessor1_Model06\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\nAfter returning the best results for hyperparameters, I found that the top five results had nearly identical mse. The minimal variability between these top performing results suggests that a wide varitey of combinations would produce good returns for the model. However, a mtry of 1 and 1167 trees produced the lowest overall mse, meaning that it is the most reliable result.\n\nrf_final_wf &lt;- finalize_workflow(\n  rf_wf_cc,\n  hp_best\n)\n\n\n\nFinal Model Verification\n\nrf_final_fit &lt;- last_fit(\n  rf_final_wf,\n  split = cc_split\n)\n\ncollect_metrics(rf_final_fit)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.603 Preprocessor1_Model1\n2 rsq     standard       0.799 Preprocessor1_Model1\n\npredictions &lt;- collect_predictions(rf_final_fit)\n\nggplot(predictions, aes(x = .pred, y = logQmean)) +\n  geom_point(aes(color = logQmean), alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_abline(linetype = \"dashed\", color = \"red\") +       \n  scale_color_viridis_c() +   \n  labs(\n    title = \"Predicted vs Actual logQmean Values\",\n    x = \"Predicted Value\",\n    y = \"Actual Value\",\n    color = \"Actual Value\"\n  ) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAgainst the test data, there is a strong correlation between the models predicted data and the actual data. Most of the plots on the graph align with the line, indicating that while there is some prediction error at extreme case points, majority of the predicted data was accurate.\n\n\nBuilding a Map\n\nfinal_rf_fit &lt;- fit(rf_final_wf, data = camels_clean)\n\nrf_predictions &lt;- augment(final_rf_fit, new_data = camels_clean) %&gt;%\n  mutate(residual = (.pred - logQmean)^2)\n\nprediction_map &lt;- ggplot(rf_predictions, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +\n  geom_point(size = 1.5) +\n  scale_color_viridis_c(option = \"plasma\") +\n  coord_fixed(1.3) +\n  labs(title = \"Predicted logQmean Values\", color = \"Prediction\") +\n  theme_bw()\n\nresid_map &lt;- ggplot(rf_predictions, aes(x = gauge_lon, y = gauge_lat, color = residual)) +\n  geom_point(size = 1.5) +\n  scale_color_viridis_c(option = \"magma\") +\n  coord_fixed(1.3) +\n  labs(title = \"Prediction Residuals (Squared)\", color = \"Residual²\") +\n  theme_bw()\n\nprediction_map + resid_map"
  }
]